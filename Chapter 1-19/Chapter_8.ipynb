{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb953e6",
   "metadata": {},
   "source": [
    "# Chapter 8: Dimensionality Reduction\n",
    "\n",
    "Notebook ini merupakan hasil reproduksi dan penjelasan teori dari **Bab 8 - Dimensionality Reduction** dari buku *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Edition)* oleh AurÃ©lien GÃ©ron.\n",
    "\n",
    "ðŸ“Œ Fokus bab ini adalah memahami alasan dan cara mereduksi dimensi data, serta penggunaan teknik populer seperti PCA dan LLE.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667632dd",
   "metadata": {},
   "source": [
    "## Ringkasan Teori Bab 8: Dimensionality Reduction\n",
    "\n",
    "### 1. Apa Itu Dimensionality Reduction?\n",
    "\n",
    "Dimensionality Reduction adalah proses mengurangi jumlah fitur dalam dataset, tanpa kehilangan terlalu banyak informasi penting.\n",
    "\n",
    "Alasan mengapa dilakukan:\n",
    "- Mempercepat training\n",
    "- Mengurangi overfitting\n",
    "- Memvisualisasikan data (misalnya 2D atau 3D)\n",
    "- Menghilangkan redundansi atau noise\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Proyeksi vs. Manifold Learning\n",
    "\n",
    "- **Proyeksi**: Mereduksi dimensi dengan memproyeksikan data ke ruang berdimensi lebih rendah.\n",
    "- **Manifold Learning**: Mengasumsikan data berada pada manifold berdimensi lebih rendah, dan mencoba memetakan struktur tersebut.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA menemukan arah (komponen utama) dengan varians terbesar. Komponen ini digunakan untuk merepresentasikan data.\n",
    "\n",
    "Langkah-langkah PCA:\n",
    "1. Pusatkan data (zero mean)\n",
    "2. Hitung covariance matrix\n",
    "3. Temukan eigenvectors (principal components)\n",
    "4. Proyeksikan data ke komponen utama\n",
    "\n",
    "**Scikit-learn:**\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Menentukan Jumlah Komponen\n",
    "\n",
    "Gunakan atribut `explained_variance_ratio_` dari PCA untuk menentukan jumlah komponen yang mempertahankan persentase informasi (varians) tertentu.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Incremental PCA\n",
    "\n",
    "Untuk dataset besar, PCA dapat dilakukan secara bertahap (batch-wise) menggunakan `IncrementalPCA`.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Kernel PCA\n",
    "\n",
    "Digunakan untuk menangkap struktur non-linear dalam data dengan kernel trick.\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import KernelPCA\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Local Linear Embedding (LLE)\n",
    "\n",
    "Teknik **manifold learning** yang menjaga hubungan linear lokal antar titik data.\n",
    "\n",
    "LLE mempertahankan **koefisien linear** antar tetangga saat mereduksi dimensi.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Perbandingan Singkat Teknik\n",
    "| Teknik | Cocok Untuk | Karakteristik |\n",
    "|-------|-------------|----------------|\n",
    "| PCA | Data linear | Cepat dan umum |\n",
    "| Kernel PCA | Non-linear | Lebih fleksibel |\n",
    "| LLE | Non-linear manifold | Menjaga hubungan lokal |\n",
    "| t-SNE | Visualisasi | Tidak cocok untuk prediksi |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184de573",
   "metadata": {},
   "source": [
    "## Implementasi PCA dan Visualisasi 2D\n",
    "\n",
    "Contoh menggunakan PCA untuk mereduksi dimensi dataset iris dari 4D ke 2D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\"viridis\", edgecolor=\"k\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA: Iris Dataset Reduced to 2D\")\n",
    "plt.colorbar(label=\"Target Class\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1469bd",
   "metadata": {},
   "source": [
    "## Penjelasan Visualisasi PCA\n",
    "\n",
    "- Setiap titik mewakili satu instance dari dataset iris.\n",
    "- Warna menunjukkan kelas target (setosa, versicolor, virginica).\n",
    "- PCA mencari dua arah terbaik (principal components) yang mempertahankan informasi sebanyak mungkin dari fitur asli.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12caea11",
   "metadata": {},
   "source": [
    "## Implementasi Kernel PCA\n",
    "\n",
    "Menggunakan kernel RBF (Gaussian) untuk mengungkap struktur non-linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a836bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.1)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y, cmap=\"plasma\", edgecolor=\"k\")\n",
    "plt.xlabel(\"KPCA 1\")\n",
    "plt.ylabel(\"KPCA 2\")\n",
    "plt.title(\"Kernel PCA with RBF Kernel\")\n",
    "plt.colorbar(label=\"Target Class\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d712d85",
   "metadata": {},
   "source": [
    "## Penjelasan Visualisasi Kernel PCA\n",
    "\n",
    "- Kernel PCA mampu mengungkap pola yang tidak dapat ditemukan oleh PCA biasa.\n",
    "- Dengan kernel RBF, kita menangkap struktur yang melengkung dan non-linear.\n",
    "- Cocok untuk dataset di mana hubungan antar fitur tidak linier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0995c23",
   "metadata": {},
   "source": [
    "## Implementasi LLE (Local Linear Embedding)\n",
    "\n",
    "Menjaga hubungan lokal tetangga dalam ruang berdimensi lebih rendah.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d669c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_lle = lle.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_lle[:, 0], X_lle[:, 1], c=y, cmap=\"coolwarm\", edgecolor=\"k\")\n",
    "plt.xlabel(\"LLE 1\")\n",
    "plt.ylabel(\"LLE 2\")\n",
    "plt.title(\"Local Linear Embedding (LLE)\")\n",
    "plt.colorbar(label=\"Target Class\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4348d",
   "metadata": {},
   "source": [
    "## Penjelasan Visualisasi LLE\n",
    "\n",
    "- LLE memproyeksikan data ke 2D sambil mempertahankan hubungan lokal.\n",
    "- Cocok untuk manifold learning, terutama untuk data dengan struktur kompleks.\n",
    "- Titik-titik dari kelas yang sama cenderung berkumpul meskipun bentuknya tidak linear.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
