{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6023bd06",
   "metadata": {},
   "source": [
    "# Chapter 5: Support Vector Machines\n",
    "\n",
    "Notebook ini merupakan hasil reproduksi dan penjelasan teori dari **Bab 5 - Support Vector Machines (SVM)** dari buku *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Edition)* oleh AurÃ©lien GÃ©ron.\n",
    "\n",
    "ðŸ“Œ Fokus bab ini adalah klasifikasi margin maksimum menggunakan Support Vector Classifier (SVC), serta konsep margin, kernel trick, dan regularisasi dalam SVM.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a08b0",
   "metadata": {},
   "source": [
    "## Ringkasan Teori Bab 5: Support Vector Machines\n",
    "\n",
    "### 1. Konsep Dasar\n",
    "SVM bertujuan mencari hyperplane terbaik yang memisahkan dua kelas dengan **margin maksimum**.\n",
    "\n",
    "### 2. Margin\n",
    "- **Hard Margin**: memisahkan data tanpa kesalahan â€” hanya cocok untuk data yang bisa dipisahkan sempurna.\n",
    "- **Soft Margin**: memungkinkan sedikit pelanggaran (misalnya outlier) untuk menghindari overfitting.\n",
    "\n",
    "### 3. Kernel Trick\n",
    "Digunakan untuk mengubah data ke dimensi yang lebih tinggi agar bisa dipisahkan secara linear.\n",
    "\n",
    "Beberapa kernel umum:\n",
    "- Linear\n",
    "- Polynomial: $K(x, x') = (x^T x' + r)^d$\n",
    "- RBF (Gaussian): $K(x, x') = \\exp(-\\gamma ||x - x'||^2)$\n",
    "\n",
    "### 4. Hyperparameter\n",
    "- $C$: mengontrol trade-off antara margin dan error (regularisasi)\n",
    "- $\\gamma$: pada RBF kernel, mengontrol seberapa jauh pengaruh satu sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd077f44",
   "metadata": {},
   "source": [
    "## Contoh Klasifikasi dengan SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2bf86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate data\n",
    "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "# Pipeline SVM dengan RBF kernel\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\", SVC(kernel=\"rbf\", gamma=2, C=1))\n",
    "])\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36064207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, ax=None):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "    ax.set_title(\"Decision Boundary SVM\")\n",
    "\n",
    "plot_decision_boundary(svm_clf, X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcae608",
   "metadata": {},
   "source": [
    "## Linear SVM Classifier\n",
    "\n",
    "Untuk data yang dapat dipisahkan secara linear, kita bisa menggunakan `LinearSVC` yang lebih efisien.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_lin, y_lin = make_blobs(n_samples=100, centers=2, random_state=42)\n",
    "\n",
    "lin_svc = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svm\", LinearSVC(C=1, loss=\"hinge\", max_iter=10000))\n",
    "])\n",
    "lin_svc.fit(X_lin, y_lin)\n",
    "\n",
    "plot_decision_boundary(lin_svc, X_lin, y_lin)\n",
    "plt.title(\"Linear SVM Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc51d50",
   "metadata": {},
   "source": [
    "## SVM untuk Regresi: SVR\n",
    "\n",
    "SVM juga bisa digunakan untuk regresi (SVR - Support Vector Regression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2664f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Data regresi\n",
    "np.random.seed(42)\n",
    "X_r = 2 * np.random.rand(100, 1)\n",
    "y_r = np.sin(X_r).ravel() + np.random.randn(100) * 0.1\n",
    "\n",
    "svr_model = SVR(kernel='rbf', C=100, gamma=0.5, epsilon=0.1)\n",
    "svr_model.fit(X_r, y_r)\n",
    "\n",
    "X_r_test = np.linspace(0, 2, 100).reshape(-1, 1)\n",
    "y_r_pred = svr_model.predict(X_r_test)\n",
    "\n",
    "plt.plot(X_r, y_r, \"b.\", label=\"Training data\")\n",
    "plt.plot(X_r_test, y_r_pred, \"r-\", label=\"SVR prediction\")\n",
    "plt.title(\"Support Vector Regression\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8802083f",
   "metadata": {},
   "source": [
    "## Kesimpulan\n",
    "\n",
    "- SVM sangat kuat untuk klasifikasi margin maksimum dan bisa diperluas untuk data non-linear dengan kernel.\n",
    "- Untuk dataset besar, `SGDClassifier` dengan loss=\"hinge\" bisa digunakan sebagai alternatif yang lebih ringan.\n",
    "- SVR cocok untuk regresi kompleks tetapi mahal secara komputasi.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
