{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976bd987",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Networks\n",
    "\n",
    "Notebook ini merupakan hasil reproduksi dan penjelasan teori dari **Bab 11 - Training Deep Neural Networks** dari buku *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Edition)* oleh AurÃ©lien GÃ©ron.\n",
    "\n",
    "ðŸ“Œ Bab ini membahas teknik-teknik penting dalam melatih Deep Neural Networks (DNN) secara efisien dan stabil, termasuk regularisasi, optimisasi, dan inisialisasi bobot.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f1557",
   "metadata": {},
   "source": [
    "## Ringkasan Teori Bab 11: Training Deep Neural Networks\n",
    "\n",
    "### 1. Tantangan dalam Melatih Deep Neural Networks\n",
    "\n",
    "- **Vanishing/Exploding Gradients**: Gradien terlalu kecil atau besar menyebabkan training gagal\n",
    "- **Overfitting**: Model terlalu kompleks dan menyesuaikan noise dari data pelatihan\n",
    "- **Training lambat atau tidak konvergen**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Strategi Inisialisasi Bobot\n",
    "\n",
    "- **Glorot/Xavier Initialization**: ideal untuk sigmoid/tanh\n",
    "- **He Initialization**: cocok untuk ReLU\n",
    "```python\n",
    "keras.layers.Dense(..., kernel_initializer=\"he_normal\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Normalisasi Fitur\n",
    "\n",
    "Sebelum training, sangat penting untuk menormalisasi fitur agar distribusi input seragam.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Batch Normalization\n",
    "\n",
    "Teknik ini menormalkan output dari layer sebelum fungsi aktivasi, mempercepat training dan membantu stabilitas.\n",
    "\n",
    "```python\n",
    "keras.layers.BatchNormalization()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Regularisasi\n",
    "\n",
    "Menghindari overfitting:\n",
    "- **Dropout**: menonaktifkan neuron secara acak saat training\n",
    "```python\n",
    "keras.layers.Dropout(0.5)\n",
    "```\n",
    "- **L1/L2 Regularization**: ditambahkan ke fungsi loss\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Optimizers\n",
    "\n",
    "Algoritma untuk memperbarui bobot:\n",
    "- **SGD (Stochastic Gradient Descent)**\n",
    "- **Momentum**\n",
    "- **RMSProp**\n",
    "- **Adam**: sangat populer dan efisien\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Early Stopping dan Callback\n",
    "\n",
    "Menghentikan training jika validation loss berhenti membaik.\n",
    "\n",
    "```python\n",
    "keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Learning Rate Scheduling\n",
    "\n",
    "Menurunkan learning rate saat training berlangsung.\n",
    "\n",
    "```python\n",
    "keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Simulasi Gradient Clipping\n",
    "\n",
    "Membatasi besar gradien untuk menghindari exploding gradients.\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.Adam(clipnorm=1.0)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460431f",
   "metadata": {},
   "source": [
    "## Implementasi DNN dengan Regularisasi dan Callbacks\n",
    "\n",
    "Berikut adalah implementasi Deep Neural Network (DNN) menggunakan teknik:\n",
    "- **Batch Normalization**\n",
    "- **Dropout**\n",
    "- **Adam Optimizer dengan Gradient Clipping**\n",
    "- **EarlyStopping & ReduceLROnPlateau callbacks**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b911025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load dan siapkan data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "y_cat = to_categorical(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model DNN dengan regulasi\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation=\"relu\", kernel_initializer=\"he_normal\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01, clipnorm=1.0)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79348b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback: EarlyStopping dan Learning Rate Scheduler\n",
    "early_stop = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=100, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9c79c",
   "metadata": {},
   "source": [
    "## Evaluasi Model dan Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4de0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27bdc0b",
   "metadata": {},
   "source": [
    "### Penjelasan:\n",
    "\n",
    "- **Batch Normalization** mempercepat konvergensi dan menstabilkan training.\n",
    "- **Dropout** mencegah overfitting dengan menonaktifkan neuron secara acak.\n",
    "- **EarlyStopping** menghentikan training jika tidak ada perbaikan.\n",
    "- **ReduceLROnPlateau** menurunkan learning rate otomatis saat stagnasi.\n",
    "- **Gradient Clipping** menghindari exploding gradients dengan membatasi nilai gradien maksimum.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
