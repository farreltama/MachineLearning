{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3d1e71",
   "metadata": {},
   "source": [
    "# Chapter 15: Distributing TensorFlow Across Devices and Servers\n",
    "\n",
    "Notebook ini merupakan hasil reproduksi dan penjelasan teori dari **Bab 15 - Distributing TensorFlow Across Devices and Servers** dari buku *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Edition)* oleh AurÃ©lien GÃ©ron.\n",
    "\n",
    "ðŸ“Œ Bab ini membahas cara melakukan distribusi training model TensorFlow menggunakan beberapa perangkat (GPU/TPU) atau bahkan server yang berbeda.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4ee26",
   "metadata": {},
   "source": [
    "## Ringkasan Teori Bab 15: Distributing TensorFlow Across Devices and Servers\n",
    "\n",
    "### 1. Mengapa Perlu Distribusi?\n",
    "\n",
    "- Model deep learning skala besar membutuhkan waktu lama untuk dilatih\n",
    "- Dataset besar sulit ditangani oleh satu GPU\n",
    "- Distribusi membantu mempercepat training dan mengurangi bottleneck\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Deteksi Perangkat\n",
    "\n",
    "```python\n",
    "tf.config.list_physical_devices()\n",
    "```\n",
    "\n",
    "TensorFlow secara otomatis dapat menggunakan CPU, GPU, atau TPU jika tersedia.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Strategi Distribusi\n",
    "\n",
    "TensorFlow menyediakan beberapa strategi:\n",
    "\n",
    "- **`MirroredStrategy`**: Salin model ke beberapa GPU di satu mesin\n",
    "- **`MultiWorkerMirroredStrategy`**: Distribusi antar beberapa worker/machine\n",
    "- **`TPUStrategy`**: Untuk menjalankan di TPU (Google Cloud, Colab)\n",
    "- **`CentralStorageStrategy`**: Parameter disimpan di host dan dikirim ke worker saat training\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Penggunaan `MirroredStrategy`\n",
    "\n",
    "```python\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = keras.models.Sequential([...])\n",
    "    model.compile(...)\n",
    "```\n",
    "\n",
    "- Parameter model disalin ke semua GPU\n",
    "- Gradien digabungkan dan dibagi otomatis\n",
    "- Kompatibel dengan `.fit()`\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Dataset Sharding Otomatis\n",
    "\n",
    "Saat menggunakan strategi distribusi, dataset secara otomatis akan di-*shard* (dibagi) ke tiap worker agar efisien dan sinkron.\n",
    "\n",
    "```python\n",
    "dataset = dataset.batch(32)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. MultiWorkerMirroredStrategy\n",
    "\n",
    "Gunakan `TF_CONFIG` environment variable untuk mendeskripsikan cluster (beberapa mesin):\n",
    "```json\n",
    "{\n",
    "  \"cluster\": {\n",
    "    \"worker\": [\"host1:port\", \"host2:port\"]\n",
    "  },\n",
    "  \"task\": {\"type\": \"worker\", \"index\": 0}\n",
    "}\n",
    "```\n",
    "\n",
    "- Butuh komunikasi antar node\n",
    "- Cocok untuk cloud/enterprise training\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Tips Distribusi:\n",
    "\n",
    "- Jangan gunakan `.fit()` tanpa `strategy.scope()`!\n",
    "- Simpan model di dalam `scope`\n",
    "- Gunakan `strategy.run(fn)` jika menulis loop manual\n",
    "- Gunakan GPU yang identik untuk sinkronisasi optimal\n",
    "\n",
    "---\n",
    "\n",
    "### 8. TPU Strategy (Opsional)\n",
    "\n",
    "```python\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "```\n",
    "\n",
    "- Biasanya hanya tersedia di Google Colab Pro atau GCP TPU\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“Œ Ringkasan:\n",
    "Distribusi model memungkinkan training skala besar secara efisien, baik di laptop multi-GPU, server, atau cloud TPU/GPU farms.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387ec49",
   "metadata": {},
   "source": [
    "## Implementasi Distribusi dengan MirroredStrategy\n",
    "\n",
    "Strategi ini secara otomatis membagi data dan mereplikasi model ke semua GPU dalam satu mesin. Cocok digunakan di Colab Pro atau PC dengan banyak GPU.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7564e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Dummy dataset\n",
    "X = np.random.rand(1000, 10)\n",
    "y = (X[:, 0] + X[:, 1] * 2 + np.random.rand(1000)) > 1.5\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "# Gunakan strategi distribusi\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Jumlah perangkat: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "# Buat dan latih model di dalam scope\n",
    "with strategy.scope():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(32, activation='relu', input_shape=(10,)),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Latih model\n",
    "model.fit(X, y, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5cca6b",
   "metadata": {},
   "source": [
    "### Penjelasan:\n",
    "- `MirroredStrategy()` secara otomatis mencari semua GPU yang tersedia\n",
    "- `strategy.scope()` wajib digunakan saat membuat dan melatih model\n",
    "- Model akan didistribusikan secara sinkron (gradien digabung dan dibagi rata)\n",
    "- Training bisa dipercepat 2x, 4x, atau lebih tergantung jumlah GPU\n",
    "\n",
    "ðŸ“Œ Gunakan GPU identik agar sinkronisasi tidak menjadi bottleneck.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
