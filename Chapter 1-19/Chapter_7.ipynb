{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd32233",
   "metadata": {},
   "source": [
    "# Chapter 7: Ensemble Learning and Random Forests\n",
    "\n",
    "Notebook ini merupakan hasil reproduksi dan penjelasan teori dari **Bab 7 - Ensemble Learning and Random Forests** dari buku *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Edition)* oleh AurÃ©lien GÃ©ron.\n",
    "\n",
    "ðŸ“Œ Bab ini menjelaskan konsep model ensemble, termasuk voting, bagging, Random Forests, boosting, dan teknik lain yang digunakan untuk meningkatkan performa prediksi.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce64314f",
   "metadata": {},
   "source": [
    "## Ringkasan Teori Bab 7: Ensemble Learning and Random Forests\n",
    "\n",
    "### 1. Apa Itu Ensemble Learning?\n",
    "\n",
    "**Ensemble Learning** adalah teknik menggabungkan prediksi dari beberapa model untuk meningkatkan akurasi.  \n",
    "Alih-alih mengandalkan satu model yang kuat, kita menggunakan banyak model lemah (weak learners) secara bersamaan.\n",
    "\n",
    "**Jenis umum ensemble:**\n",
    "- **Bagging (Bootstrap Aggregating)**: melatih model yang sama pada subset acak data\n",
    "- **Boosting**: melatih model secara berurutan, dengan fokus pada kesalahan sebelumnya\n",
    "- **Voting**: menggabungkan prediksi beberapa model (klasifikasi)\n",
    "- **Stacking**: menggunakan model lain (meta-learner) untuk menggabungkan output model dasar\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Voting Classifier\n",
    "\n",
    "- **Hard Voting**: mayoritas suara dari masing-masing model\n",
    "- **Soft Voting**: rata-rata probabilitas prediksi, lalu pilih probabilitas tertinggi\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Bagging dan Pasting\n",
    "\n",
    "- **Bagging**: sampling *dengan* pengembalian (bootstrap sampling)\n",
    "- **Pasting**: sampling *tanpa* pengembalian\n",
    "\n",
    "Keduanya dapat digunakan untuk mengurangi varians model.\n",
    "\n",
    "Scikit-learn:\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Random Forest\n",
    "\n",
    "Random Forest = banyak Decision Tree yang dilatih secara bagging, ditambah pemilihan fitur acak di setiap split.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "```\n",
    "\n",
    "Kelebihan:\n",
    "- Kuat terhadap overfitting\n",
    "- Bisa menghitung feature importance\n",
    "- Cepat dan paralel\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Boosting\n",
    "\n",
    "Boosting melatih model secara berurutan, di mana setiap model mencoba memperbaiki kesalahan dari model sebelumnya.\n",
    "\n",
    "Dua pendekatan populer:\n",
    "- **AdaBoost**: penyesuaian bobot pada setiap sampel\n",
    "- **Gradient Boosting**: model baru dilatih untuk memprediksi *residual error* dari model sebelumnya\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Feature Importance dari Random Forest\n",
    "\n",
    "Kita bisa mengukur pentingnya setiap fitur berdasarkan berapa banyak impurity yang berhasil dikurangi oleh fitur tersebut secara rata-rata di seluruh pohon.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Out-of-Bag Evaluation (OOB)\n",
    "\n",
    "Pada Bagging, kita dapat menggunakan instance yang tidak dipilih (out-of-bag) untuk mengevaluasi model, tanpa perlu validation set eksplisit.\n",
    "\n",
    "```python\n",
    "BaggingClassifier(oob_score=True)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb90ab1",
   "metadata": {},
   "source": [
    "## Implementasi: Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
    "\n",
    "log_clf = LogisticRegression(max_iter=1000)\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('dt', tree_clf), ('svc', svm_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "for clf in (log_clf, tree_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc43b3bb",
   "metadata": {},
   "source": [
    "## Implementasi: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b29616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "print(\"Random Forest accuracy:\", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba0d80",
   "metadata": {},
   "source": [
    "## Visualisasi Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "importances = rf_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "names = [iris.feature_names[i] for i in indices]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_train.shape[1]), names, rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75fca95",
   "metadata": {},
   "source": [
    "## Implementasi: Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9670840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_pred_gb = gb_clf.predict(X_test)\n",
    "print(\"Gradient Boosting accuracy:\", accuracy_score(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9058b62e",
   "metadata": {},
   "source": [
    "## Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0edd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=100,\n",
    "    bootstrap=True, oob_score=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "print(\"OOB Score:\", bag_clf.oob_score_)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
